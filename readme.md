# Simple AVA: A Simplified VAE for Animal Vocalizations

This project provides a minimal, self-contained, and easy-to-use implementation of the Autoencoded Vocal Analysis (AVA) framework. It is designed to train a Variational Autoencoder (VAE) on animal vocalization data with minimal dependencies and a straightforward workflow.

The VAE network architecture and loss function are faithful reproductions of the original implementation, ensuring high-quality latent representations.

### Features
- **Minimal Dependencies**: Only requires essential libraries like PyTorch, NumPy, and SciPy.
- **Clear Workflow**: Three separate scripts for a clear, step-by-step process: `preprocess`, `train`, and `extract_latents`.
- **Flexible Data Handling**: Easily handles multiple audio files and corresponding train/test CSV annotations.
- **Original Architecture**: Uses the exact VAE network from the original AVA paper for proven performance.

## Quick Start: One-Liner Setup

This guide assumes you have Python 3.8+ and `pip` installed.

First, install `uv`, a fast Python package installer:
```bash
pip install uv
```

Now, from your project directory (`simple_ava/`), run the commands for your operating system.

### For macOS & Linux

You can create the environment and install dependencies in one line. Then, activate it with the second line.

```bash
# 1. Create environment and install all packages
uv venv && uv pip install -r requirements.txt

# 2. Activate the environment (run this every time you open a new terminal)
source .venv/bin/activate
```

### For Windows (Command Prompt)

You can create the environment and install dependencies in one line. Then, activate it with the second line.

```bash
:: 1. Create environment and install all packages
uv venv && uv pip install -r requirements.txt

:: 2. Activate the environment (run this every time you open a new terminal)
.venv\Scripts\activate
```

Your environment is now ready. You can proceed to the **Workflow & Usage** section.

## Project Structure

To use this project, your files should be organized as follows:

```
simple_ava/
├── data/
│   ├── BIRD1.csv           # Training segments for BIRD1.wav
│   ├── BIRD1_test.csv      # (Optional) Testing segments for BIRD1.wav
│   ├── BIRD1.wav           # Audio file
│   ├── BIRD2.csv           # Training segments for BIRD2.wav
│   └── BIRD2.wav
│
├── processed_data/         # Auto-generated by preprocess.py
│   ├── train/
│   │   ├── BIRD1_train.hdf5
│   │   └── BIRD2_train.hdf5
│   └── test/
│       └── BIRD1_test.hdf5
│
├── model_checkpoints/      # Auto-generated by train.py
│   ├── checkpoint_020.tar
│   └── ...
│
├── requirements.txt        # Project dependencies
├── preprocess.py           # Script to process audio into spectrograms
├── train.py                # Script to train the VAE model
└── extract_latents.py      # Script to get latent vectors from a trained model
```

## Workflow & Usage

The process is broken down into three simple steps.

### Step 0: Prepare Your Data

Place your audio (`.wav`) and annotation (`.csv`) files in the `data/` directory.

-   **Audio Files**: Must be in `.wav` format.
-   **CSV Files**: Must contain at least `onset` and `offset` columns with timestamps in seconds. The header should look like this:
    ```csv
    onset,offset,minFrequency,maxFrequency,...
    0.210,0.359,432,22050,...
    0.414,0.450,1384,20061,...
    ```
-   **Naming Convention**: For an audio file named `MY_BIRD.wav`, the corresponding training annotations should be in `MY_BIRD.csv` and optional testing annotations in `MY_BIRD_test.csv`.

### Step 1: Preprocess Audio Data

The `preprocess.py` script reads your raw data, converts each vocalization segment into a fixed-size spectrogram, and saves the results in HDF5 files.

Run the script from your terminal:
```bash
python preprocess.py
```
This will create the `processed_data/` directory, which is used for training.

### Step 2: Train the VAE Model

The `train.py` script loads the preprocessed spectrograms and trains the VAE.

Run the training script:
```bash
python train.py
```
Model checkpoints will be saved periodically to the `model_checkpoints/` directory.

### Step 3: Extract Latent Representations

After training, use the `extract_latents.py` script to get the hidden representation (the `mu` vector) for any specific audio segment. This is the primary output for your downstream analysis tasks (e.g., clustering, UMAP visualization).

**Usage:**
```bash
python extract_latents.py \
    --model_path [PATH_TO_CHECKPOINT] \
    --audio_path [PATH_TO_WAV_FILE] \
    --onset [START_TIME_SECONDS] \
    --offset [END_TIME_SECONDS]
```

**Example:**
```bash
python extract_latents.py \
    --model_path model_checkpoints/checkpoint_100.tar \
    --audio_path data/ZF.wav \
    --onset 0.210 \
    --offset 0.359```
This will print the 32-dimensional latent vector for that specific vocalization to your console.

## Configuration

Key parameters (like sample rate, spectrogram settings, and model dimensions) are located at the top of each script (`preprocess.py`, `train.py`, `extract_latents.py`). **Ensure these parameters are consistent across all files.**
